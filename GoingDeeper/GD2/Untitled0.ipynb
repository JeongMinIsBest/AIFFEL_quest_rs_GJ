{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dbVHo3QEmjg",
        "outputId": "f334de52-9d5a-4bd3-fbca-aea2ba4b24bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.19.0\n",
            "3.10.0\n",
            "0.13.2\n",
            "2.0.2\n",
            "2.2.2\n",
            "1.6.1\n"
          ]
        }
      ],
      "source": [
        "import tensorflow\n",
        "import matplotlib\n",
        "import seaborn\n",
        "import numpy\n",
        "import pandas\n",
        "import sklearn\n",
        "\n",
        "print(tensorflow.__version__)\n",
        "print(matplotlib.__version__)\n",
        "print(seaborn.__version__)\n",
        "print(numpy.__version__)\n",
        "print(pandas.__version__)\n",
        "print(sklearn.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.datasets import reuters"
      ],
      "metadata": {
        "id": "6sbvaMG_FVbJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train, y_train), (X_test, y_test) = reuters.load_data(num_words=None, test_split=0.2)\n",
        "\n",
        "print('훈련용 뉴스 기사 : {}'.format(len(X_train)))\n",
        "print('테스트용 뉴스 기사 : {}'.format(len(X_test)))\n",
        "num_classes = len(set(y_train))\n",
        "print('카테고리 : {}'.format(num_classes))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQfkdpN3GCtQ",
        "outputId": "9e9d3ad7-adbb-4286-d35c-391f8f851cd1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters.npz\n",
            "\u001b[1m2110848/2110848\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "훈련용 뉴스 기사 : 8982\n",
            "테스트용 뉴스 기사 : 2246\n",
            "카테고리 : 46\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "vocab size 로 싷험하기"
      ],
      "metadata": {
        "id": "82SslsiNNDfq"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6e9132e",
        "outputId": "e766b8a3-fa41-4125-d2bc-903152a92388"
      },
      "source": [
        "(X_train_all, y_train_all), (X_test_all, y_test_all) = reuters.load_data(num_words=None, test_split=0.2)\n",
        "print(\"Shape of X_train_all:\", X_train_all.shape)\n",
        "print(\"Shape of X_test_all:\", X_test_all.shape)\n",
        "\n",
        "(X_train_5000, y_train_5000), (X_test_5000, y_test_5000) = reuters.load_data(num_words=5000, test_split=0.2)\n",
        "print(\"Shape of X_train_5000:\", X_train_5000.shape)\n",
        "print(\"Shape of X_test_5000:\", X_test_5000.shape)\n",
        "\n",
        "(X_train_10000, y_train_10000), (X_test_10000, y_test_10000) = reuters.load_data(num_words=10000, test_split=0.2)\n",
        "print(\"Shape of X_train_10000:\", X_train_10000.shape)\n",
        "print(\"Shape of X_test_10000:\", X_test_10000.shape)\n",
        "\n",
        "(X_train_20000, y_train_20000), (X_test_20000, y_test_20000) = reuters.load_data(num_words=20000, test_split=0.2)\n",
        "print(\"Shape of X_train_20000:\", X_train_20000.shape)\n",
        "print(\"Shape of X_test_20000:\", X_test_20000.shape)\n",
        "\n",
        "(X_train_30000, y_train_30000), (X_test_30000, y_test_30000) = reuters.load_data(num_words=30000, test_split=0.2)\n",
        "print(\"Shape of X_train_30000:\", X_train_30000.shape)\n",
        "print(\"Shape of X_test_30000:\", X_test_30000.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X_train_all: (8982,)\n",
            "Shape of X_test_all: (2246,)\n",
            "Shape of X_train_5000: (8982,)\n",
            "Shape of X_test_5000: (2246,)\n",
            "Shape of X_train_10000: (8982,)\n",
            "Shape of X_test_10000: (2246,)\n",
            "Shape of X_train_20000: (8982,)\n",
            "Shape of X_test_20000: (2246,)\n",
            "Shape of X_train_30000: (8982,)\n",
            "Shape of X_test_30000: (2246,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fc7d1c1",
        "outputId": "d23f3ebe-9e49-4c66-db98-c607d69e7e24"
      },
      "source": [
        "from tensorflow.keras.utils import Sequence\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "max_sequence_length = max(max(len(seq) for seq in X_train_all),\n",
        "                          max(len(seq) for seq in X_test_all),\n",
        "                          max(len(seq) for seq in X_train_5000),\n",
        "                          max(len(seq) for seq in X_test_5000),\n",
        "                          max(len(seq) for seq in X_train_10000),\n",
        "                          max(len(seq) for seq in X_test_10000),\n",
        "                          max(len(seq) for seq in X_train_20000),\n",
        "                          max(len(seq) for seq in X_test_20000),\n",
        "                          max(len(seq) for seq in X_train_30000),\n",
        "                          max(len(seq) for seq in X_test_30000))\n",
        "\n",
        "print(\"Maximum sequence length:\", max_sequence_length)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum sequence length: 2376\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33383733",
        "outputId": "54d6d501-e1ed-4a7e-adc5-a833312cddb5"
      },
      "source": [
        "# Load data with different num_words settings\n",
        "(X_train_all, y_train_all), (X_test_all, y_test_all) = reuters.load_data(num_words=None, test_split=0.2)\n",
        "(X_train_5000, y_train_5000), (X_test_5000, y_test_5000) = reuters.load_data(num_words=5000, test_split=0.2)\n",
        "(X_train_10000, y_train_10000), (X_test_10000, y_test_10000) = reuters.load_data(num_words=10000, test_split=0.2)\n",
        "\n",
        "# Print shapes and an example to show the effect of num_words\n",
        "print(\"Data loaded with num_words=None:\")\n",
        "print(\"Shape of X_train_all:\", X_train_all.shape)\n",
        "print(\"Example X_train_all[0]:\", X_train_all[0])\n",
        "\n",
        "print(\"\\nData loaded with num_words=5000:\")\n",
        "print(\"Shape of X_train_5000:\", X_train_5000.shape)\n",
        "print(\"Example X_train_5000[0]:\", X_train_5000[0])\n",
        "\n",
        "print(\"\\nData loaded with num_words=10000:\")\n",
        "print(\"Shape of X_train_10000:\", X_train_10000.shape)\n",
        "print(\"Example X_train_10000[0]:\", X_train_10000[0])\n",
        "\n",
        "print(\"\\nData loaded with num_words=20000:\")\n",
        "print(\"Shape of X_train_20000:\", X_train_20000.shape)\n",
        "print(\"Example X_train_20000[0]:\", X_train_20000[0])\n",
        "\n",
        "print(\"\\nData loaded with num_words=30000:\")\n",
        "print(\"Shape of X_train_30000:\", X_train_30000.shape)\n",
        "print(\"Example X_train_30000[0]:\", X_train_30000[0])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaded with num_words=None:\n",
            "Shape of X_train_all: (8982,)\n",
            "Example X_train_all[0]: [1, 27595, 28842, 8, 43, 10, 447, 5, 25, 207, 270, 5, 3095, 111, 16, 369, 186, 90, 67, 7, 89, 5, 19, 102, 6, 19, 124, 15, 90, 67, 84, 22, 482, 26, 7, 48, 4, 49, 8, 864, 39, 209, 154, 6, 151, 6, 83, 11, 15, 22, 155, 11, 15, 7, 48, 9, 4579, 1005, 504, 6, 258, 6, 272, 11, 15, 22, 134, 44, 11, 15, 16, 8, 197, 1245, 90, 67, 52, 29, 209, 30, 32, 132, 6, 109, 15, 17, 12]\n",
            "\n",
            "Data loaded with num_words=5000:\n",
            "Shape of X_train_5000: (8982,)\n",
            "Example X_train_5000[0]: [1, 2, 2, 8, 43, 10, 447, 5, 25, 207, 270, 5, 3095, 111, 16, 369, 186, 90, 67, 7, 89, 5, 19, 102, 6, 19, 124, 15, 90, 67, 84, 22, 482, 26, 7, 48, 4, 49, 8, 864, 39, 209, 154, 6, 151, 6, 83, 11, 15, 22, 155, 11, 15, 7, 48, 9, 4579, 1005, 504, 6, 258, 6, 272, 11, 15, 22, 134, 44, 11, 15, 16, 8, 197, 1245, 90, 67, 52, 29, 209, 30, 32, 132, 6, 109, 15, 17, 12]\n",
            "\n",
            "Data loaded with num_words=10000:\n",
            "Shape of X_train_10000: (8982,)\n",
            "Example X_train_10000[0]: [1, 2, 2, 8, 43, 10, 447, 5, 25, 207, 270, 5, 3095, 111, 16, 369, 186, 90, 67, 7, 89, 5, 19, 102, 6, 19, 124, 15, 90, 67, 84, 22, 482, 26, 7, 48, 4, 49, 8, 864, 39, 209, 154, 6, 151, 6, 83, 11, 15, 22, 155, 11, 15, 7, 48, 9, 4579, 1005, 504, 6, 258, 6, 272, 11, 15, 22, 134, 44, 11, 15, 16, 8, 197, 1245, 90, 67, 52, 29, 209, 30, 32, 132, 6, 109, 15, 17, 12]\n",
            "\n",
            "Data loaded with num_words=20000:\n",
            "Shape of X_train_20000: (8982,)\n",
            "Example X_train_20000[0]: [1, 2, 2, 8, 43, 10, 447, 5, 25, 207, 270, 5, 3095, 111, 16, 369, 186, 90, 67, 7, 89, 5, 19, 102, 6, 19, 124, 15, 90, 67, 84, 22, 482, 26, 7, 48, 4, 49, 8, 864, 39, 209, 154, 6, 151, 6, 83, 11, 15, 22, 155, 11, 15, 7, 48, 9, 4579, 1005, 504, 6, 258, 6, 272, 11, 15, 22, 134, 44, 11, 15, 16, 8, 197, 1245, 90, 67, 52, 29, 209, 30, 32, 132, 6, 109, 15, 17, 12]\n",
            "\n",
            "Data loaded with num_words=30000:\n",
            "Shape of X_train_30000: (8982,)\n",
            "Example X_train_30000[0]: [1, 27595, 28842, 8, 43, 10, 447, 5, 25, 207, 270, 5, 3095, 111, 16, 369, 186, 90, 67, 7, 89, 5, 19, 102, 6, 19, 124, 15, 90, 67, 84, 22, 482, 26, 7, 48, 4, 49, 8, 864, 39, 209, 154, 6, 151, 6, 83, 11, 15, 22, 155, 11, 15, 7, 48, 9, 4579, 1005, 504, 6, 258, 6, 272, 11, 15, 22, 134, 44, 11, 15, 16, 8, 197, 1245, 90, 67, 52, 29, 209, 30, 32, 132, 6, 109, 15, 17, 12]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "모델을 한곳에서 다 처리"
      ],
      "metadata": {
        "id": "5RURZsDGM-XV"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d27b9c8",
        "outputId": "129a80a5-66f4-49bb-c35d-f2a5cdfa31c5"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "results = {}\n",
        "num_words_settings_data = {\n",
        "    'all': (X_train_all, X_test_all, y_train_all, y_test_all),\n",
        "    '5000': (X_train_5000, X_test_5000, y_train_5000, y_test_5000),\n",
        "    '10000': (X_train_10000, X_test_10000, y_train_10000, y_test_10000),\n",
        "    '20000': (X_train_20000, X_test_20000, y_train_20000, y_test_20000),\n",
        "    '30000': (X_train_30000, X_test_30000, y_train_30000, y_test_30000)\n",
        "}\n",
        "\n",
        "models = {\n",
        "    'Naive Bayes': MultinomialNB(),\n",
        "    'Complement Naive Bayes': ComplementNB(),\n",
        "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
        "    'SVM': SVC(),\n",
        "    'Decision Tree': DecisionTreeClassifier(),\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'Gradient Boosting': GradientBoostingClassifier()\n",
        "}\n",
        "\n",
        "voting_clf = VotingClassifier(estimators=[\n",
        "    ('mnb', MultinomialNB()),\n",
        "    ('cnb', ComplementNB()),\n",
        "    ('lr', LogisticRegression(max_iter=1000)),\n",
        "    ('svc', SVC(probability=True)),\n",
        "    ('dt', DecisionTreeClassifier()),\n",
        "    ('rf', RandomForestClassifier()),\n",
        "    ('gb', GradientBoostingClassifier())\n",
        "], voting='soft')\n",
        "\n",
        "models['Voting'] = voting_clf\n",
        "\n",
        "\n",
        "# Need to convert the integer sequences back to text for TF-IDF Vectorization\n",
        "word_index = reuters.get_word_index()\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "\n",
        "def decode_sequence(sequence):\n",
        "    # 패딩 0, 시퀀스 시작 1, unk 2 로 처리\n",
        "    return ' '.join([reverse_word_index.get(i - 3, '?') for i in sequence])\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "for setting_name, (X_train_seq, X_test_seq, y_train, y_test) in num_words_settings_data.items():\n",
        "    print(f\"--- Processing data for {setting_name} words ---\")\n",
        "\n",
        "    X_train_text = [decode_sequence(seq) for seq in X_train_seq]\n",
        "    X_test_text = [decode_sequence(seq) for seq in X_test_seq]\n",
        "\n",
        "    print(\"Vectorizing data...\")\n",
        "    vectorizer.fit(X_train_text)\n",
        "    X_train_vec = vectorizer.transform(X_train_text)\n",
        "    X_test_vec = vectorizer.transform(X_test_text)\n",
        "    print(\"Vectorization complete.\")\n",
        "\n",
        "    results[setting_name] = {}\n",
        "    for model_name, model in models.items():\n",
        "        print(f\"\\nTraining {model_name} for {setting_name} words...\")\n",
        "        pipeline = Pipeline([\n",
        "            ('model', model)\n",
        "        ])\n",
        "        pipeline.fit(X_train_vec, y_train)\n",
        "        y_pred = pipeline.predict(X_test_vec)\n",
        "\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "        recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "        f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "\n",
        "        results[setting_name][model_name] = {\n",
        "            'accuracy': accuracy,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1_score': f1\n",
        "        }\n",
        "        print(f\"Finished {model_name} for {setting_name} words. Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "import json\n",
        "print(\"\\nPerformance Results:\")\n",
        "print(json.dumps(results, indent=4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Naive Bayes for all words...\n",
            "Finished Naive Bayes for all words.\n",
            "Training Complement Naive Bayes for all words...\n",
            "Finished Complement Naive Bayes for all words.\n",
            "Training Logistic Regression for all words...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished Logistic Regression for all words.\n",
            "Training SVM for all words...\n",
            "Finished SVM for all words.\n",
            "Training Decision Tree for all words...\n",
            "Finished Decision Tree for all words.\n",
            "Training Random Forest for all words...\n",
            "Finished Random Forest for all words.\n",
            "Training Gradient Boosting for all words...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSTM"
      ],
      "metadata": {
        "id": "OJePIBSJO7PU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import Sequence\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.datasets import reuters\n",
        "\n",
        "vocab_size = 10000 # vocab_size를 뭘로 할까요..?\n",
        "max_len = 200\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = reuters.load_data(num_words=vocab_size, test_split=0.2)\n",
        "\n",
        "X_train = pad_sequences(X_train, maxlen=max_len)\n",
        "X_test = pad_sequences(X_test, maxlen=max_len)\n",
        "\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)"
      ],
      "metadata": {
        "id": "hYHoc4n7IrJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 128\n",
        "hidden_units = 128\n",
        "num_classes = 46\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim))\n",
        "model.add(LSTM(hidden_units))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
        "history = model.fit(X_train, y_train, batch_size=128, epochs=30, callbacks=[es, mc], validation_data=(X_test, y_test))\n"
      ],
      "metadata": {
        "id": "V8Nczl7IIELC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model = load_model('best_model.h5')\n",
        "print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(X_test, y_test)[1]))\n"
      ],
      "metadata": {
        "id": "064jkoaUIxJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = range(1, len(history.history['acc']) + 1)\n",
        "plt.plot(epochs, history.history['loss'])\n",
        "plt.plot(epochs, history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "LHCldU2VIyGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DjNnEWWaK9_L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}